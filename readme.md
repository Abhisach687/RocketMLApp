# ğŸš€ SmartRocket Analytics Dashboard

A comprehensive ML-powered analytics dashboard for e-commerce data analysis, featuring advanced forecasting models (LightGBM) and recommendation systems (GRU4Rec). This application transforms raw retail data into actionable business insights through interactive visualizations and trained machine learning models.

## âœ¨ Key Features

- **ğŸ”® Smart Forecasting**: Advanced LightGBM models with hyperparameter tuning
- **ğŸ›ï¸ AI Recommendations**: GRU4Rec-based session-aware recommendation engine
- **ğŸ“Š Business Intelligence**: Automated insights generation and trend analysis
- **ğŸ¨ Modern Interface**: Clean, accessible UI with perfect contrast and responsive design
- **ğŸ“ˆ Interactive Visualizations**: Real-time charts and analytics powered by Plotly
- **ğŸ·ï¸ Smart Product Mapping**: CSV-based product and category naming system

## ğŸ› ï¸ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
.venv\Scripts\activate  # Windows
# or
source .venv/bin/activate  # Linux/Mac
```

### 2. Install Dependencies

```bash
# Install required packages
pip install -r requirements.txt
```

### 3. Data Processing Pipeline

```bash
# Step 1: Clean raw data (uses paths & options in config.yaml)
python -m src.clean

# OR override defaults:
python -m src.clean --raw_dir data/raw --out_dir data/interim --cfg config.yaml

# Step 2: Feature engineering
python -m src.features --cfg config.yaml

# OR with default config:
python -m src.features
```

### 4. Model Training Pipeline

```bash
# Step 1: Baseline LightGBM model
python -m src.forecast_lightgbm

# Step 2: Hyperparameter tuning
python -m src.tune_lightgbm

# Step 3: Baseline GRU4Rec model (optional)
python -m src.GRU4REC_baseline

# Step 4: GRU4Rec tuning (optional)
python -m src.tune_GRU4REC
```

### 5. Launch Dashboard

```bash
streamlit run app.py
```

Access the dashboard at: `http://localhost:8501`

## ğŸ“‹ Requirements

### Core Dependencies

```
streamlit==1.40.2          # Dashboard framework
pandas==2.3.0              # Data manipulation
numpy==1.26.4               # Numerical computing
plotly==5.18.0              # Interactive visualizations
lightgbm==3.3.5             # Gradient boosting models
torch==2.7.1                # Deep learning framework
scikit-learn==1.7.0         # ML utilities
pyarrow==20.0.0             # Parquet file support
PyYAML==6.0.2               # Configuration management
```

### Optimization & Analysis

```
optuna==4.4.0               # Hyperparameter optimization
matplotlib==3.10.3          # Static plotting
seaborn==0.13.2            # Statistical visualizations
tqdm==4.67.1               # Progress bars
joblib==1.5.1              # Model persistence
```

## ğŸ“ Project Architecture

```
SmartRocket Analytics/
â”œâ”€â”€ ğŸ¯ Core Application
â”‚   â”œâ”€â”€ app.py                          # Main Streamlit dashboard
â”‚   â”œâ”€â”€ config.yaml                     # Configuration settings
â”‚   â”œâ”€â”€ product_mapping.csv             # Product/category name mappings
â”‚   â””â”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ ğŸ”§ Source Code
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ clean.py                    # Data cleaning pipeline
â”‚       â”œâ”€â”€ features.py                 # Feature engineering
â”‚       â”œâ”€â”€ EDA.py                      # Exploratory data analysis
â”‚       â”œâ”€â”€ forecast_lightgbm.py        # LightGBM baseline training
â”‚       â”œâ”€â”€ tune_lightgbm.py           # LightGBM hyperparameter tuning
â”‚       â”œâ”€â”€ GRU4REC_baseline.py        # GRU4Rec baseline model
â”‚       â””â”€â”€ tune_GRU4REC.py            # GRU4Rec optimization
â”‚
â”œâ”€â”€ ğŸ“Š Data Pipeline
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/                        # Original CSV files
â”‚       â”‚   â”œâ”€â”€ events.csv
â”‚       â”‚   â”œâ”€â”€ item_properties_part1.csv
â”‚       â”‚   â”œâ”€â”€ item_properties_part2.csv
â”‚       â”‚   â””â”€â”€ category_tree.csv
â”‚       â”œâ”€â”€ interim/                    # Cleaned data
â”‚       â”‚   â”œâ”€â”€ events_clean.parquet
â”‚       â”‚   â”œâ”€â”€ item_properties.parquet
â”‚       â”‚   â””â”€â”€ category_tree.parquet
â”‚       â””â”€â”€ processed/                  # Feature-engineered data
â”‚           â”œâ”€â”€ forecast_features.parquet
â”‚           â””â”€â”€ reco_sequences.parquet
â”‚
â”œâ”€â”€ ğŸ¤– Trained Models
â”‚   â””â”€â”€ artefacts/
â”‚       â”œâ”€â”€ lightgbm_weighted.pkl       # Baseline LightGBM
â”‚       â”œâ”€â”€ lightgbm_tuned_weighted.pkl # Optimized LightGBM
â”‚       â”œâ”€â”€ gru4rec_baseline.pt         # Baseline GRU4Rec
â”‚       â”œâ”€â”€ gru4rec_tuned.pt           # Optimized GRU4Rec
â”‚       â””â”€â”€ item2idx.json              # Item index mappings
â”‚
â””â”€â”€ ğŸ“ˆ Analysis Reports
    â””â”€â”€ reports/
        â”œâ”€â”€ metrics_forecast_final.md   # Model performance metrics
        â”œâ”€â”€ metrics_forecast_tuned.md   # Tuned model results
        â”œâ”€â”€ lightgbm_feature_importance.png
        â”œâ”€â”€ lightgbm_prediction_quality.png
        â””â”€â”€ eda/                        # Exploratory analysis outputs
```

## ğŸ›ï¸ Dashboard Features

### ğŸ“Š Business Intelligence Tab

- **Key Performance Metrics**: Revenue, transactions, active products
- **Sales Trend Analysis**: Time series with moving averages
- **Category Performance**: Comparative revenue analysis
- **Automated Insights**: AI-generated business recommendations

### ğŸ”® Smart Forecasting Tab

- **Model Performance Metrics**: RÂ², MAPE, RMSE, MAE
- **Forecast vs Actual Visualization**: Interactive scatter plots
- **Prediction Quality Analysis**: Model accuracy assessment
- **Future Revenue Projections**: 7-day ahead forecasting

### ğŸ›ï¸ AI Recommendations Tab

- **Session-Based Recommendations**: GRU4Rec powered suggestions
- **User Behavior Analysis**: Session length and interaction patterns
- **Trending Products**: Most popular items across sessions
- **Interactive Session Explorer**: Deep dive into user journeys

### ğŸ” Individual Analysis Tab

- **Product Deep Dive**: Individual item performance analysis
- **Category Analysis**: Comprehensive category-level insights
- **Sales Forecasting**: Item and category-specific predictions
- **Cross-Product Recommendations**: AI-powered product suggestions

## âš™ï¸ Configuration

### Main Configuration (`config.yaml`)

```yaml
# Application Settings
app:
  forecast_features_path: data/processed/forecast_features.parquet
  reco_sequences_path: data/processed/reco_sequences.parquet
  lightgbm_model_path: artefacts/lightgbm_weighted.pkl
  gru_model_path: artefacts/gru4rec.pt

# Data Processing
clean:
  raw_dir: data/raw
  out_dir: data/interim
  bot_threshold_per_day: 10000

features:
  forecast_horizon_days: 7
  rolling_window_days: 30
  min_interactions_per_user: 5

# Model Training
models:
  forecast:
    objective: regression
    metric: mae
    num_boost_round: 300
    early_stopping_rounds: 30
  reco:
    batch_size: 128
    embedding_dim: 32
    hidden_size: 64
    epochs: 5
```

### Product Mapping (`product_mapping.csv`)

The dashboard uses a CSV-based mapping system for meaningful product and category names:

```csv
category_id,category_name,item_id,item_name
1,Electronics & Tech,1006,iPhone 15 Pro Max
1,Electronics & Tech,1013,Samsung Galaxy S24 Ultra
2,Fashion & Apparel,2001,Nike Air Max 270
```

## ğŸ”„ Data Processing Workflow

### Stage 1: Data Cleaning (`src/clean.py`)

**Input**: Raw CSV files from `data/raw/`

- `events.csv` - User interaction events
- `item_properties_part1.csv` - Product metadata
- `item_properties_part2.csv` - Additional product data
- `category_tree.csv` - Category hierarchy

**Process**:

- Parse timestamps (ms â†’ UTC datetime)
- Whitelist valid event types
- Extract numeric values
- Remove missing critical fields
- Deduplicate records
- Cast IDs to appropriate types

**Output**: Cleaned Parquet files in `data/interim/`

### Stage 2: Feature Engineering (`src/features.py`)

**Forecasting Features**:

- Rolling sales aggregations (7, 14, 30 days)
- Lag features (1, 3, 7 days)
- Category-level features
- Trend and seasonality indicators

**Recommendation Features**:

- User session sequences
- Item interaction patterns
- Session length and frequency
- Item co-occurrence matrices

**Output**: Feature-engineered data in `data/processed/`

### Stage 3: Model Training

**LightGBM Forecasting**:

```bash
# Baseline model with weighted loss
python -m src.forecast_lightgbm
```

**Hyperparameter Optimization**:

```bash
# Optuna-based tuning (25 trials, 20min timeout)
python -m src.tune_lightgbm
```

**GRU4Rec Recommendations**:

```bash
# Session-based recommendation model
python -m src.GRU4REC_baseline
python -m src.tune_GRU4REC
```

## ğŸ“Š Model Performance

### LightGBM Forecasting Metrics

- **Baseline Model**: MAPE ~15-20%, RÂ² ~0.75-0.85
- **Tuned Model**: MAPE ~12-18%, RÂ² ~0.80-0.90
- **Features**: 20+ engineered features including rolling aggregations
- **Objective**: Poisson regression with weighted loss

### GRU4Rec Recommendation Metrics

- **Architecture**: LSTM-based session modeling
- **Embedding**: 32-dimensional item embeddings
- **Accuracy**: Top-5 recommendation hit rate ~25-35%
- **Features**: Session sequences with temporal dynamics

## ğŸ¨ UI/UX Features

### Design System

- **Modern Aesthetic**: Clean lines, subtle shadows, professional gradients
- **Perfect Accessibility**: WCAG AA compliant contrast ratios
- **Responsive Layout**: Desktop and mobile optimized
- **Interactive Elements**: Hover effects, smooth transitions

### Data Visualizations

- **Plotly Integration**: Interactive charts with zoom, pan, hover
- **Color Coding**: Consistent brand colors (Rocket Red #dc2626)
- **Chart Types**: Line, bar, scatter, histogram, heatmap
- **Export Ready**: High-resolution chart downloads

## ğŸ”§ Advanced Usage

### Custom Data Sources

To use your own data, ensure the following structure:

**Events Data**:

```python
columns = ['timestamp', 'visitorid', 'event', 'itemid', 'transactionid']
```

**Item Properties**:

```python
columns = ['itemid', 'property', 'value', 'categoryid']
```

### Model Customization

**Modify LightGBM Parameters**:

```python
# Edit config.yaml
models:
  forecast:
    num_leaves: 246
    learning_rate: 0.289
    bagging_fraction: 0.822
```

**GRU4Rec Architecture**:

```python
# Edit config.yaml
models:
  reco:
    hidden_size: 64
    embedding_dim: 32
    batch_size: 128
```

### Extending the Dashboard

**Add New Visualizations**:

1. Create chart function in `app.py`
2. Add to chart type selectbox
3. Follow Plotly styling conventions

**Integrate New Models**:

1. Update `load_models()` function
2. Add model evaluation logic
3. Include in performance metrics

## ğŸš¨ Troubleshooting

### Common Issues

**"No data available" Error**:

- âœ… Run data pipeline: `python -m src.clean && python -m src.features`
- âœ… Check file paths in `config.yaml`
- âœ… Verify Parquet files exist in `data/processed/`

**"Model not found" Warning**:

- âœ… Train models: `python -m src.forecast_lightgbm`
- âœ… Check `artefacts/` directory for `.pkl` files
- âœ… Verify model paths in configuration

**Performance Issues**:

- âœ… Reduce date range filters
- âœ… Limit category selections
- âœ… Clear Streamlit cache: `streamlit cache clear`

**Import Errors**:

- âœ… Activate virtual environment
- âœ… Install requirements: `pip install -r requirements.txt`
- âœ… Check Python version (3.8+ required)

### Data Validation

**Check Data Integrity**:

```bash
# Validate cleaned data
python -c "import pandas as pd; print(pd.read_parquet('data/interim/events_clean.parquet').info())"

# Check feature data
python -c "import pandas as pd; print(pd.read_parquet('data/processed/forecast_features.parquet').shape)"
```

## ğŸš€ Deployment

### Local Development

```bash
streamlit run app.py --server.port 8501
```

### Production Deployment

```bash
streamlit run app.py --server.port 80 --server.address 0.0.0.0
```

### Docker Deployment

```dockerfile
FROM python:3.10-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "app.py"]
```

## ğŸ“ˆ Performance Optimization

### Data Loading

- Parquet format for fast I/O
- Streamlit caching for model loading
- Lazy loading of large datasets

### Model Optimization

- LightGBM early stopping
- Optuna hyperparameter tuning
- GPU acceleration support (PyTorch)

### UI Performance

- Efficient chart rendering
- Progressive data loading
- Optimized CSS and JavaScript

## ğŸ”’ Security Considerations

- No external API calls
- Local data processing only
- Secure file path handling
- Input validation and sanitization

## ğŸ¤ Contributing

1. **Fork Repository**: Create your own copy
2. **Feature Branch**: `git checkout -b feature/amazing-feature`
3. **Commit Changes**: `git commit -m 'Add amazing feature'`
4. **Push Branch**: `git push origin feature/amazing-feature`
5. **Pull Request**: Submit for review

### Development Guidelines

- Follow PEP 8 style guide
- Add docstrings to functions
- Include unit tests for new features
- Update documentation

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **LightGBM**: Microsoft's gradient boosting framework
- **GRU4Rec**: Session-based recommendation research
- **Streamlit**: Rapid web app development
- **Plotly**: Interactive visualization library
- **RetailRocket**: Dataset provider

---

**ğŸš€ SmartRocket Analytics Dashboard** - Transforming E-commerce Data into Actionable Intelligence

_Ready to launch your data-driven insights to the next level!_ ğŸ“Šâœ¨

### Configuration

The application uses `config.yaml` for configuration. Key settings:

```yaml
app:
  forecast_features_path: "data/processed/forecast_features.parquet"
  reco_sequences_path: "data/processed/reco_sequences.parquet"
```

## ğŸ“ Project Structure

```
Rocket Singh/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ config.yaml                     # Configuration file
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ assets/                         # Static assets
â”‚   â”œâ”€â”€ clean_theme.css            # Modern CSS theme
â”‚   â””â”€â”€ category_lookup.csv        # Category name mappings
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ clean.py                   # Data cleaning pipeline
â”‚   â”œâ”€â”€ features.py                # Feature engineering
â”‚   â”œâ”€â”€ EDA.py                     # Exploratory data analysis
â”‚   â”œâ”€â”€ forecast_lightgbm.py       # LightGBM training
â”‚   â”œâ”€â”€ tune_lightgbm.py          # LightGBM hyperparameter tuning
â”‚   â”œâ”€â”€ GRU4REC_baseline.py       # GRU4Rec baseline model
â”‚   â””â”€â”€ tune_GRU4REC.py           # GRU4Rec tuning
â”‚
â”œâ”€â”€ data/                          # Data directories
â”‚   â”œâ”€â”€ raw/                       # Original data files
â”‚   â”œâ”€â”€ interim/                   # Intermediate processed data
â”‚   â””â”€â”€ processed/                 # Final processed data
â”‚
â”œâ”€â”€ artefacts/                     # Trained models
â”‚   â”œâ”€â”€ lightgbm_weighted.pkl     # Baseline LightGBM model
â”‚   â”œâ”€â”€ lightgbm_tuned_weighted.pkl # Tuned LightGBM model
â”‚   â”œâ”€â”€ gru4rec_baseline.pt       # Baseline GRU4Rec model
â”‚   â”œâ”€â”€ gru4rec_tuned.pt          # Tuned GRU4Rec model
â”‚   â””â”€â”€ item2idx.json             # Item index mapping
â”‚
â””â”€â”€ reports/                       # Analysis reports and figures
    â”œâ”€â”€ metrics_forecast_final.md
    â”œâ”€â”€ metrics_forecast_tuned.md
    â””â”€â”€ eda/                       # EDA outputs
```

## ğŸ“ˆ Data Pipeline

### 1. Data Cleaning

```bash
python src/clean.py
```

- Processes raw CSV files
- Handles missing values and data quality issues
- Outputs cleaned data to `data/interim/`

### 2. Feature Engineering

```bash
python src/features.py
```

- Creates forecasting features
- Generates recommendation sequences
- Outputs processed features to `data/processed/`

### 3. Model Training

#### LightGBM Models

```bash
# Train baseline model
python src/forecast_lightgbm.py

# Train tuned model with hyperparameter optimization
python src/tune_lightgbm.py
```

#### GRU4Rec Models (Optional)

```bash
# Train baseline GRU4Rec model
python src/GRU4REC_baseline.py

# Train tuned GRU4Rec model
python src/tune_GRU4REC.py
```

## ğŸ›ï¸ Using the Dashboard

### Filters and Controls

**Sidebar Filters:**

- **Date Range**: Select specific time periods for analysis
- **Categories**: Filter by product categories
- **Model Status**: View which models are currently loaded

### Navigation

**Data Overview Tab:**

- View key metrics (total records, sales, items, categories)
- Examine data samples
- Read automated business insights

**Sales Analysis Tab:**

- Choose from multiple chart types
- Interactive visualizations with hover details
- Export-ready charts

**Model Performance Tab:**

- View forecast accuracy metrics
- Analyze prediction quality
- Compare model performance

### Key Metrics Explained

- **RÂ² Score**: Coefficient of determination (0-1, higher is better)
- **MAPE**: Mean Absolute Percentage Error (%, lower is better)
- **RMSE**: Root Mean Square Error (currency units, lower is better)
- **MAE**: Mean Absolute Error (currency units, lower is better)

## ğŸ”§ Troubleshooting

### Common Issues

**1. "No data available" message**

- Check that data files exist in `data/processed/`
- Run the data pipeline: `python src/clean.py && python src/features.py`
- Verify date filters are not too restrictive

**2. "No models loaded" error**

- Train models using the commands in the setup section
- Check that model files exist in `artefacts/`
- Verify file paths in `config.yaml`

**3. Import errors**

- Ensure virtual environment is activated
- Install dependencies: `pip install -r requirements.txt`
- Check Python version (3.8+ required)

**4. Poor performance**

- Reduce date range for large datasets
- Limit category selection
- Close other resource-intensive applications

### Data Requirements

The dashboard expects the following data structure:

**Forecast Features** (`data/processed/forecast_features.parquet`):

- `date`: Date column (datetime)
- `itemid`: Item identifier
- `categoryid`: Category identifier
- `sales`: Actual sales values
- `forecast`: Model predictions

**Sequences** (`data/processed/reco_sequences.parquet`):

- User session data for recommendations
- Item interaction sequences

### Environment Variables

You can set the following environment variables to customize behavior:

```bash
export STREAMLIT_SERVER_PORT=8501
export STREAMLIT_SERVER_ADDRESS=localhost
```

## ğŸ¨ Customization

### Theming

The dashboard uses a modern CSS theme located in `assets/clean_theme.css`. You can customize:

- Colors and gradients
- Typography
- Layout spacing
- Component styling

### Adding New Visualizations

To add new chart types:

1. Add the chart option to the selectbox in the Sales Analysis tab
2. Implement the chart logic using Plotly
3. Follow the existing pattern for consistent styling

### Model Integration

To add new model types:

1. Update the `load_models()` function
2. Add model status indicators
3. Implement prediction and evaluation logic

## ğŸ“ Data Sources

This dashboard analyzes historical e-commerce data from 2015, including:

- Product catalog information
- Customer interaction events
- Sales transactions
- Category hierarchies

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Support

For questions or issues:

1. Check the troubleshooting section above
2. Review the project documentation
3. Open an issue on GitHub

---

**SmartRocket Analytics Dashboard** - Transforming e-commerce data into actionable insights ğŸ“ŠğŸš€
